{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicasilvacodes/pln-pucminas/blob/main/pln_07_representa%C3%A7%C3%A3o_textual_pucminas_JessicaSilva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representação Textual"
      ],
      "metadata": {
        "id": "Di-8PxNrHNym"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfMtJgRJAC2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46f4c55-a3d5-4e81-f385-c52e8e78e3c1"
      },
      "source": [
        "# instalação dos pacotes necessários\n",
        "!pip install --upgrade --no-cach  e-dir numpy scikit-learn\n",
        "!pip install nltk\n",
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6V8u14WAC26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a02fc66-777b-49bc-9f88-2c04efca8048"
      },
      "source": [
        "# Importações da biblioteca padrão\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "# Importações de bibliotecas de terceiros\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Downloads do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Configurações e comandos específicos (por exemplo, desativar avisos)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Pacotes importados com sucesso; notebook pronto para uso!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pacotes importados com sucesso; notebook pronto para uso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# declara as funções utilitárias do notebook\n",
        "def formata_msg(nivel, msg):\n",
        "  \"\"\"\n",
        "    Formata uma mensagem de log incluindo o nível de severidade, timestamp\n",
        "    e a mensagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - nivel (str): Nível de severidade da mensagem (ex: 'INFO', 'ERROR', 'WARNING').\n",
        "    - msg (str): A mensagem de log propriamente dita.\n",
        "\n",
        "    Retorna:\n",
        "    - str: A mensagem de log formatada.\n",
        "  \"\"\"\n",
        "\n",
        "  timestamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "  return f\"[{nivel}] {timestamp} - {msg}\"\n",
        "\n",
        "\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Funções utilitárias prontas para utilização.\"))\n",
        "print(formata_msg(\"INFO\", f\"Versão do Python: {sys.version} \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVzUjrWHNRs0",
        "outputId": "eb30b940-5a4a-4a0e-e3d8-b1ebb851ff61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2025-08-01 - Funções utilitárias prontas para utilização.\n",
            "[INFO] 2025-08-01 - Versão do Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2ZMw4zfAC3A"
      },
      "source": [
        "# Definição do Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentos =\\\n",
        "[\n",
        "  # No Meio do Caminho - Carlos Drumond de Andrade\n",
        "  \"No meio do caminho tinha uma pedra\\n\"\\\n",
        "  \"tinha uma pedra no meio do caminho\\n\"\\\n",
        "  \"tinha uma pedra\\n\"\\\n",
        "  \"no meio do caminho tinha uma pedra.\\n\"\\\n",
        "  \"Nunca me esquecerei desse acontecimento\\n\"\\\n",
        "  \"na vida de minhas retinas tão fatigadas.\\n\"\\\n",
        "  \"Nunca me esquecerei que no meio do caminho\\n\"\\\n",
        "  \"tinha uma pedra\\n\"\\\n",
        "  \"tinha uma pedra no meio do caminho\\n\"\\\n",
        "  \"no meio do caminho tinha uma pedra.\"\n",
        "  ,\n",
        "  # Quadrilha - Carlos Drumond de Andrade\n",
        "  \"João amava Teresa que amava Raimundo\\n\"\\\n",
        "  \"que amava Maria que amava Joaquim que amava Lili,\\n\"\\\n",
        "  \"que não amava ninguém.\\n\"\\\n",
        "  \"João foi para os Estados Unidos, Teresa para o convento,\\n\"\\\n",
        "  \"Raimundo morreu de desastre, Maria ficou para tia,\\n\"\\\n",
        "  \"Joaquim suicidou-se e Lili casou com J. Pinto Fernandes\\n\"\\\n",
        "  \"que não tinha entrado na história.\"\n",
        " ]"
      ],
      "metadata": {
        "id": "-1dik4Cpsn4I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formata_msg(\"INFO\", f\"Documentos:\\n{documentos}\"))"
      ],
      "metadata": {
        "id": "mj3U06pyzSkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f85e3e-7a83-4b67-96a2-a344b1d655ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2025-08-01 - Documentos:\n",
            "['No meio do caminho tinha uma pedra\\ntinha uma pedra no meio do caminho\\ntinha uma pedra\\nno meio do caminho tinha uma pedra.\\nNunca me esquecerei desse acontecimento\\nna vida de minhas retinas tão fatigadas.\\nNunca me esquecerei que no meio do caminho\\ntinha uma pedra\\ntinha uma pedra no meio do caminho\\nno meio do caminho tinha uma pedra.', 'João amava Teresa que amava Raimundo\\nque amava Maria que amava Joaquim que amava Lili,\\nque não amava ninguém.\\nJoão foi para os Estados Unidos, Teresa para o convento,\\nRaimundo morreu de desastre, Maria ficou para tia,\\nJoaquim suicidou-se e Lili casou com J. Pinto Fernandes\\nque não tinha entrado na história.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1vjVVoYAC3T"
      },
      "source": [
        "## Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_MuzVllAC3U"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "1) Escreva uma método que realiza o pré-processamento da lista de <b>documentos</b>.\n",
        "\n",
        "O método deve, para cada documento:\n",
        "- tokenizar cada palavra\n",
        "- remover stopwords\n",
        "- remover números\n",
        "- remover pontuções\n",
        "- remover acentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsfgYeVIa9PX"
      },
      "source": [
        "def pre_processa_texto(texto):\n",
        "    \"\"\"\n",
        "    Preprocessa o texto fornecido realizando várias etapas de limpeza.\n",
        "\n",
        "    Etapas:\n",
        "    1. Tokeniza o texto.\n",
        "    2. Converte os tokens para minúsculos.\n",
        "    3. Remove stopwords em português.\n",
        "    4. Remove números dos tokens.\n",
        "    5. Exclui tokens que são pontuações.\n",
        "    6. Remove acentuações dos tokens.\n",
        "\n",
        "    Parâmetros:\n",
        "    texto (str): O texto a ser preprocessado.\n",
        "\n",
        "    Retorna:\n",
        "    list: Lista de tokens preprocessados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokeniza o texto usando um padrão para capturar palavras e pontuações.\n",
        "    padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "    tokens_preprocessados = re.findall(padrao, texto)\n",
        "\n",
        "    # Converte os tokens para minúsculos para padronizar a capitalização.\n",
        "    tokens_preprocessados = [token.lower() for token in tokens_preprocessados]\n",
        "\n",
        "    # Remove stopwords para reduzir o conjunto de tokens a palavras significativas.\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in portugues_stops]\n",
        "\n",
        "    # Remove números, pois geralmente não contribuem para o significado do texto.\n",
        "    tokens_preprocessados = [re.sub(r'\\d+', '', token) for token in tokens_preprocessados\n",
        "                             if re.sub(r'\\d+', '', token)]\n",
        "\n",
        "    # Exclui tokens que são pontuações, pois raramente são úteis para análise de texto.\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in string.punctuation]\n",
        "\n",
        "    # Remove acentuações para padronizar os tokens.\n",
        "    tokens_preprocessados = [unidecode(token) for token in tokens_preprocessados]\n",
        "\n",
        "    return ' '.join(tokens_preprocessados)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_preprocessados =  [pre_processa_texto(poema)  for poema in documentos]\n",
        "print(formata_msg(\"INFO\", f\"Documentos preprocessados:\\n{documentos_preprocessados}\"))"
      ],
      "metadata": {
        "id": "3FmIHc8XbK1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1577a5ea-667c-4e00-c13d-48714d0edf44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2025-08-01 - Documentos preprocessados:\n",
            "['meio caminho pedra pedra meio caminho pedra meio caminho pedra nunca esquecerei desse acontecimento vida retinas tao fatigadas nunca esquecerei meio caminho pedra pedra meio caminho meio caminho pedra', 'joao amava teresa amava raimundo amava maria amava joaquim amava lili amava ninguem joao estados unidos teresa convento raimundo morreu desastre maria ficou tia joaquim suicidou lili casou j pinto fernandes entrado historia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-ZevNeAC4-"
      },
      "source": [
        "## Bag of Words\n",
        "\n",
        "Faça o CountVectorizer nos documentos da variável <b>documentos</b> considerando binary = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z8_t3vabNMi"
      },
      "source": [
        "def aplica_bow(documentos_preprocessados):\n",
        "    \"\"\"\n",
        "    Analisa documentos usando BoW e retorna o vocabulário extraído e um\n",
        "    DataFrame contendo os valores BoW para cada termo em cada documento.\n",
        "\n",
        "    Parâmetros:\n",
        "    - documentos_preprocessados: Lista de documentos preprocessados (strings).\n",
        "\n",
        "    Retorna:\n",
        "    - Um dicionário contendo o vocabulário extraído e o DataFrame dos valores\n",
        "      BoW.\n",
        "    \"\"\"\n",
        "    # Inicializa o CountVectorizer\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "    # Transforma os documentos em uma matriz binária\n",
        "    bow_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "    # Obtém o vocabulário extraído\n",
        "    vocabulario = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Cria um DataFrame para os valores bow.\n",
        "    df = pd.DataFrame(\n",
        "        bow_matrix.T.todense(),\n",
        "        index=vocabulario,\n",
        "        columns=[f\"doc{i + 1}\" for i in range(bow_matrix.shape[0])]\n",
        "    )\n",
        "\n",
        "    return {\"vocabulario\": vocabulario, \"bow_dataframe\": df}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_dict = aplica_bow(documentos_preprocessados)\n",
        "print(formata_msg(\"INFO\", f\"Vocabulário BoW:\\n{bow_dict['vocabulario']}\"))\n",
        "print(formata_msg(\"INFO\", f\"Dataframe BoW:\\n{bow_dict['bow_dataframe']}\"))"
      ],
      "metadata": {
        "id": "Qm2foixK1jHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f40405-fa3d-4c82-b6cb-3a08c0bb4294"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2025-08-01 - Vocabulário BoW:\n",
            "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
            " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
            " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
            " 'nunca' 'pedra' 'pinto' 'raimundo' 'retinas' 'suicidou' 'tao' 'teresa'\n",
            " 'tia' 'unidos' 'vida']\n",
            "[INFO] 2025-08-01 - Dataframe BoW:\n",
            "               doc1  doc2\n",
            "acontecimento     1     0\n",
            "amava             0     1\n",
            "caminho           1     0\n",
            "casou             0     1\n",
            "convento          0     1\n",
            "desastre          0     1\n",
            "desse             1     0\n",
            "entrado           0     1\n",
            "esquecerei        1     0\n",
            "estados           0     1\n",
            "fatigadas         1     0\n",
            "fernandes         0     1\n",
            "ficou             0     1\n",
            "historia          0     1\n",
            "joao              0     1\n",
            "joaquim           0     1\n",
            "lili              0     1\n",
            "maria             0     1\n",
            "meio              1     0\n",
            "morreu            0     1\n",
            "ninguem           0     1\n",
            "nunca             1     0\n",
            "pedra             1     0\n",
            "pinto             0     1\n",
            "raimundo          0     1\n",
            "retinas           1     0\n",
            "suicidou          0     1\n",
            "tao               1     0\n",
            "teresa            0     1\n",
            "tia               0     1\n",
            "unidos            0     1\n",
            "vida              1     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFHut-GWAC4L"
      },
      "source": [
        "## TD-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZD0n75gAC4M"
      },
      "source": [
        "1) Faça o TDIFTVectorizer nos documentos da variável <b>documentos</b> sem alterar nenhum parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WMh8XBbClS"
      },
      "source": [
        "def aplica_tfidf(documentos_preprocessados):\n",
        "    \"\"\"\n",
        "    Analisa documentos usando TF-IDF e retorna o vocabulário extraído e um\n",
        "    DataFrame contendo os valores TF-IDF para cada termo em cada documento.\n",
        "\n",
        "    Parâmetros:\n",
        "    - documentos_preprocessados: Lista de documentos preprocessados (strings).\n",
        "\n",
        "    Retorna:\n",
        "    - Um dicionário contendo o vocabulário extraído e o DataFrame dos valores\n",
        "      TF-IDF.\n",
        "    \"\"\"\n",
        "    # Inicializa o TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Transforma os documentos em uma matriz TF-IDF\n",
        "    tfidf_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "    # Obtém o vocabulário extraído\n",
        "    vocabulario = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Cria um DataFrame para os valores TF-IDF\n",
        "    df = pd.DataFrame(\n",
        "        tfidf_matrix.T.todense(),\n",
        "        index=vocabulario,\n",
        "        columns=[f\"doc{i + 1}\" for i in range(tfidf_matrix.shape[0])]\n",
        "    )\n",
        "\n",
        "    return {\"vocabulario\": vocabulario, \"tfidf_dataframe\": df}\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  tfidf_dict = aplica_tfidf(documentos_preprocessados)\n",
        "  print(formata_msg(\"INFO\", f\"Vocabulário TF-IDF:\\n{tfidf_dict['vocabulario']}\"))\n",
        "  print(formata_msg(\"INFO\", f\"Dataframe TF-IDF:\\n{tfidf_dict['tfidf_dataframe']}\"))"
      ],
      "metadata": {
        "id": "rF5IqhPqxpBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6de38a3-9c9c-4b12-9655-1016857772bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2025-08-01 - Vocabulário TF-IDF:\n",
            "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
            " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
            " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
            " 'nunca' 'pedra' 'pinto' 'raimundo' 'retinas' 'suicidou' 'tao' 'teresa'\n",
            " 'tia' 'unidos' 'vida']\n",
            "[INFO] 2025-08-01 - Dataframe TF-IDF:\n",
            "                   doc1      doc2\n",
            "acontecimento  0.086066  0.000000\n",
            "amava          0.000000  0.697486\n",
            "caminho        0.516398  0.000000\n",
            "casou          0.000000  0.116248\n",
            "convento       0.000000  0.116248\n",
            "desastre       0.000000  0.116248\n",
            "desse          0.086066  0.000000\n",
            "entrado        0.000000  0.116248\n",
            "esquecerei     0.172133  0.000000\n",
            "estados        0.000000  0.116248\n",
            "fatigadas      0.086066  0.000000\n",
            "fernandes      0.000000  0.116248\n",
            "ficou          0.000000  0.116248\n",
            "historia       0.000000  0.116248\n",
            "joao           0.000000  0.232495\n",
            "joaquim        0.000000  0.232495\n",
            "lili           0.000000  0.232495\n",
            "maria          0.000000  0.232495\n",
            "meio           0.516398  0.000000\n",
            "morreu         0.000000  0.116248\n",
            "ninguem        0.000000  0.116248\n",
            "nunca          0.172133  0.000000\n",
            "pedra          0.602464  0.000000\n",
            "pinto          0.000000  0.116248\n",
            "raimundo       0.000000  0.232495\n",
            "retinas        0.086066  0.000000\n",
            "suicidou       0.000000  0.116248\n",
            "tao            0.086066  0.000000\n",
            "teresa         0.000000  0.232495\n",
            "tia            0.000000  0.116248\n",
            "unidos         0.000000  0.116248\n",
            "vida           0.086066  0.000000\n"
          ]
        }
      ]
    }
  ]
}